# ğŸ§  RAG Local â€” A Minimal Local Retrieval-Augmented Generation Demo

This project demonstrates a **fully local Retrieval-Augmented Generation (RAG)** pipeline using Python, [LangChain](https://python.langchain.com/), [ChromaDB](https://www.trychroma.com/), and [Ollama](https://ollama.com/).

The goal is to **learn RAG end-to-end** â€” from document loading and embedding to retrieval and LLM-based question answering â€” with everything running **locally** on a Mac.

---

## ğŸš€ Features

- ğŸ“„ Loads all documents (`.pdf`, `.txt`, `.md`) from a local `docs/` folder  
- ğŸ§© Splits documents into text chunks for efficient search  
- ğŸ” Stores embeddings in a local [ChromaDB](https://www.trychroma.com/) vector database  
- ğŸ¤– Uses [Ollama](https://ollama.com/) for both **LLM** and **embedding models** (no API key required)  
- ğŸ’¬ Lets you ask natural-language questions and get answers based on your own documents  
- ğŸ—‚ï¸ Everything runs locally â€” ideal for learning or offline experimentation  

---

## ğŸ§° Requirements

- macOS or Linux  
- Python 3.10+  
- [Ollama](https://ollama.com/download) installed and running locally  

---

## ğŸ“¦ Installation

```bash
# 1. Clone this repository
git clone https://github.com/russ-hendy/rag-local.git
cd rag-local

# 2. Create and activate a virtual environment
python3 -m venv venv
source venv/bin/activate  # (on macOS/Linux)

# 3. Install dependencies
pip install -r requirements.txt
````

---

## ğŸ¦™ Set Up Ollama

Install Ollama if you havenâ€™t already:

ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

Then pull the models used in this project:

```bash
ollama pull llama3
ollama pull nomic-embed-text
```

You can verify it works:

```bash
ollama run llama3
```

---

## ğŸ“‚ Add Some Documents

Put your `.pdf`, `.txt`, or `.md` files in the `docs/` folder, e.g.:

```
rag-local/
  â”œâ”€â”€ docs/
  â”‚     â”œâ”€â”€ article1.pdf
  â”‚     â”œâ”€â”€ notes.txt
  â”œâ”€â”€ rag.py
  â”œâ”€â”€ requirements.txt
  â””â”€â”€ README.md
```

You can use any text-rich sources (books, reports, articles, etc.).

---

## â–¶ï¸ Run the RAG Script

```bash
python rag.py
```

Youâ€™ll see:

```
âœ… Loaded 4 documents from docs
ğŸ§© Split into 172 chunks

ğŸ’¬ Ask a question: 
```

Type your question, for example:

```
ğŸ’¬ Ask a question: What are the main themes discussed in these documents?
```

Then watch your local LLM respond based on your retrieved document context.

---

## ğŸ§± Project Structure

```
rag-local/
â”‚
â”œâ”€â”€ docs/                 # Local documents youâ€™ll embed & search
â”œâ”€â”€ chroma_db/            # Local persisted vector database (auto-created)
â”œâ”€â”€ rag.py                # Main RAG pipeline script
â”œâ”€â”€ requirements.txt      # Python dependencies (pinned)
â””â”€â”€ README.md             # This file
```

---

## ğŸ§© How It Works

1. **Load Documents** â†’ All files in `docs/` are loaded via `PyPDFLoader` or `TextLoader`.
2. **Chunking** â†’ Text is split into small overlapping chunks for better retrieval.
3. **Embedding** â†’ Each chunk is embedded into a high-dimensional vector using `nomic-embed-text`.
4. **Vector Store** â†’ Chunks + embeddings are saved locally in `ChromaDB`.
5. **Query** â†’ When you ask a question, the most similar chunks are retrieved.
6. **LLM Generation** â†’ `llama3` uses the retrieved context to generate a grounded answer.

---

## âš¡ Optional: Use OpenAI Instead of Ollama

If youâ€™d rather test using an OpenAI model for inference:

```bash
pip install langchain-openai openai
export OPENAI_API_KEY="sk-..."
```

Then replace in `rag.py`:

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

embedding = OpenAIEmbeddings(model="text-embedding-3-small")
llm = ChatOpenAI(model="gpt-4o-mini")
```

---

## ğŸ§  Next Steps

* Add metadata (source filenames) to retrieved chunks
* Build a Streamlit UI for interactive RAG
* Try hybrid search (keyword + vector)
* Experiment with different chunk sizes or embeddings
* Compare Ollama vs OpenAI performance

---

## ğŸªª License

MIT License Â© 2025 Russ Hendy
This project is for educational and experimental use.

---
